import numpy as np
import matplotlib.pyplot as plt

# Define a function to generate sample sine wave data
def generate_sine_wave_data(num_samples, seq_len, num_features):
    x = np.linspace(0, 50, seq_len)
    data = []
    for _ in range(num_samples):
        noise = np.random.normal(0, 0.1, size=seq_len)
        sine_wave = np.sin(x) + noise
        data.append(sine_wave)
    data = np.array(data).reshape(num_samples, seq_len, num_features)
    return data

# Generate sample data
num_samples = 100
seq_len = 20
num_features = 1

data = generate_sine_wave_data(num_samples, seq_len, num_features)
data = torch.tensor(data, dtype=torch.float32)

# Instantiate the autoencoder
autoencoder = Autoencoder(input_size, hidden_size, latent_size, seq_len)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    optimizer.zero_grad()
    output = autoencoder(data)
    loss = criterion(output, data)
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Get a sample input and its reconstruction
sample_input = data[0].unsqueeze(0)
sample_output = autoencoder(sample_input).detach().numpy().squeeze()

# Plot the original and reconstructed time series
plt.figure(figsize=(12, 6))
plt.plot(sample_input.numpy().squeeze(), label='Original')
plt.plot(sample_output, label='Reconstructed')
plt.legend()
plt.title('Original vs Reconstructed Time Series')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.show()




import torch
import torch.nn as nn
import torch.optim as optim

# Define the Encoder class
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, latent_size):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, latent_size)
    
    def forward(self, x):
        _, (h, _) = self.lstm(x)
        latent = self.fc(h[-1])
        return latent

# Define the Decoder class
class Decoder(nn.Module):
    def __init__(self, latent_size, hidden_size, output_size):
        super(Decoder, self).__init__()
        self.fc = nn.Linear(latent_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, output_size, batch_first=True)
    
    def forward(self, x, seq_len):
        hidden = self.fc(x).unsqueeze(0)
        hidden = hidden.repeat(1, seq_len, 1)
        output, _ = self.lstm(hidden)
        return output

# Define the Autoencoder class that combines Encoder and Decoder
class Autoencoder(nn.Module):
    def __init__(self, input_size, hidden_size, latent_size, seq_len):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder(input_size, hidden_size, latent_size)
        self.decoder = Decoder(latent_size, hidden_size, input_size)
        self.seq_len = seq_len
    
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent, self.seq_len)
        return reconstructed

# Hyperparameters
input_size = 1  # Number of features in the input
hidden_size = 64  # Number of features in the hidden state
latent_size = 32  # Size of the latent vector
seq_len = 20  # Length of the input sequences

# Instantiate the autoencoder
autoencoder = Autoencoder(input_size, hidden_size, latent_size, seq_len)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

# Dummy input data (batch_size, seq_len, input_size)
data = torch.randn(10, seq_len, input_size)

# Training loop (for demonstration purposes, here we use only one epoch)
for epoch in range(1):
    optimizer.zero_grad()
    output = autoencoder(data)
    loss = criterion(output, data)
    loss.backward()
    optimizer.step()
    print(f'Epoch [{epoch+1}], Loss: {loss.item():.4f}')
